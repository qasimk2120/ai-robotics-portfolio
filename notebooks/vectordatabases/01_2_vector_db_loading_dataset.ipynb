{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa74fb8",
   "metadata": {},
   "source": [
    "# Large-Scale Text Embeddings with Pinecone and Sentence Transformers\n",
    "\n",
    "This notebook demonstrates how to build a **large-scale text vector index**\n",
    "using **Pinecone** and **Sentence Transformers**.\n",
    "\n",
    "The workflow includes:\n",
    "- Streaming a large text dataset\n",
    "- Generating embeddings locally\n",
    "- Creating a Pinecone index\n",
    "- Upserting vectors in batches\n",
    "\n",
    "A small subset of data is used to keep the example practical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656427e",
   "metadata": {},
   "source": [
    "## Setup and Authentication\n",
    "\n",
    "Before interacting with Pinecone, we must:\n",
    "- Load API credentials from environment variables\n",
    "- Initialize the Pinecone client\n",
    "\n",
    "A local embedding model is used to generate vectors\n",
    "before sending them to the vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key= os.environ.get(\"PINECONE_API_KEY\"), environment= os.environ.get(\"PINECONE_ENV\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50562442",
   "metadata": {},
   "source": [
    "## Loading a Large Text Dataset\n",
    "\n",
    "The **FineWeb** dataset is loaded using Hugging Face Datasets.\n",
    "\n",
    "Streaming mode is enabled to:\n",
    "- Avoid downloading the full dataset\n",
    "- Process items incrementally\n",
    "- Scale to very large corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw =  load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0da34",
   "metadata": {},
   "source": [
    "## Inspecting Dataset Features\n",
    "\n",
    "Understanding the dataset schema helps identify\n",
    "which fields should be embedded and stored as metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f631ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f04e73",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "A Sentence Transformer model is used to convert text into vectors.\n",
    "\n",
    "`all-MiniLM-L6-v2` is:\n",
    "- General-purpose\n",
    "- Fast\n",
    "- Low-dimensional\n",
    "- Suitable for large-scale indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4759c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f7305",
   "metadata": {},
   "source": [
    "## Pinecone Index Setup\n",
    "\n",
    "The Pinecone index is created using:\n",
    "- Dimensionality derived from the embedding model\n",
    "- Cosine similarity\n",
    "- Serverless deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8803a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(name= \"text\", dimension= model.get_sentence_embedding_dimension(), metric= \"cosine\", spec= ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(name=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b04571",
   "metadata": {},
   "source": [
    "## Preparing and Upserting Text Data\n",
    "\n",
    "To efficiently index large volumes of text, the data is processed\n",
    "incrementally and upserted into Pinecone in batches.\n",
    "\n",
    "This step includes:\n",
    "- Selecting a manageable subset of the dataset\n",
    "- Generating embeddings for each text sample\n",
    "- Attaching metadata (e.g., language)\n",
    "- Upserting vectors in batches to improve throughput\n",
    "  and reduce network overhead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the number of items you want to process (subset size)\n",
    "subset_size = 10000  # For example, take only 10,000 items\n",
    "\n",
    "# Iterate over the dataset and prepare data for upserting\n",
    "vectors_to_upsert = []\n",
    "for i, item in enumerate(fw):\n",
    "    if i >= subset_size:\n",
    "        break\n",
    "\n",
    "    text = item['text']\n",
    "    unique_id = str(item['id'])\n",
    "    language = item['language']\n",
    "\n",
    "    # Create an embedding for the text\n",
    "    embedding = model.encode(text, show_progress_bar=False).tolist()\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadata = {'language': language}\n",
    "\n",
    "    # Append the tuple (id, embedding, metadata) to the list\n",
    "    vectors_to_upsert.append((unique_id, embedding, metadata))\n",
    "\n",
    "# Upsert data to Pinecone in batches\n",
    "batch_size = 1000  # Adjust based on your environment and dataset size\n",
    "for i in range(0, len(vectors_to_upsert), batch_size):\n",
    "    batch = vectors_to_upsert[i:i + batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "\n",
    "print(\"Subset of data upserted to Pinecone index.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9959a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to build a scalable\n",
    "text vector index using Pinecone:\n",
    "\n",
    "- Streaming a large dataset with Hugging Face Datasets\n",
    "- Generating embeddings using Sentence Transformers\n",
    "- Creating a Pinecone index with matching dimensionality\n",
    "- Upserting large volumes of data in batches\n",
    "\n",
    "This setup forms the foundation for\n",
    "semantic search and RAG pipelines at scale.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone-env",
   "language": "python",
   "name": "pinecone-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
