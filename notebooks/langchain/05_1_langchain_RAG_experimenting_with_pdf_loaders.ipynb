{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf4431f",
   "metadata": {},
   "source": [
    "# Steps in Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "This notebook demonstrates the **indexing phase of a Retrieval-Augmented\n",
    "Generation (RAG) pipeline**.\n",
    "\n",
    "The focus is on the core preprocessing steps required before retrieval\n",
    "and generation can occur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160f0a5",
   "metadata": {},
   "source": [
    "## RAG Indexing Pipeline Overview\n",
    "\n",
    "The indexing stage of a RAG system typically consists of the following steps:\n",
    "\n",
    "1. **Document Loading**  \n",
    "   Load raw documents from different sources (PDF, DOCX, Markdown, etc.).\n",
    "\n",
    "2. **Document Splitting**  \n",
    "   Split documents into smaller, semantically meaningful chunks\n",
    "   suitable for embedding and retrieval.\n",
    "\n",
    "3. **Document Embedding and Storage**  \n",
    "   Convert text chunks into vector embeddings that can later be stored\n",
    "   in a vector database for similarity search.\n",
    "\n",
    "This notebook focuses on **loading, splitting, and embedding**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c054f3",
   "metadata": {},
   "source": [
    "## Step 1: Document Loading\n",
    "\n",
    "Document loading is the first step in the RAG indexing pipeline.\n",
    "\n",
    "LangChain provides different loaders for handling various document formats,\n",
    "such as PDF and DOCX, while preserving metadata and page structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f920a1c",
   "metadata": {},
   "source": [
    "### Loading Documents with `PyPDFLoader`\n",
    "\n",
    "The PDF loader reads a document page by page and converts each page\n",
    "into a `Document` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_pdf = PyPDFLoader(\"../../data/docs/Introduction_to_Data_and_Data_Science.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf = loader_pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051cfe7",
   "metadata": {},
   "source": [
    "PDF text often contains excessive whitespace and line breaks.\n",
    "To improve downstream processing, the page content is normalized\n",
    "by collapsing extra spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf_cut = copy.deepcopy(pages_pdf)  #to avoid modifying the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(pages_pdf_cut[0].page_content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pages_pdf_cut:\n",
    "    i.page_content = ' '.join(i.page_content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17facb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf[0].page_content, pages_pdf_cut[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f9615",
   "metadata": {},
   "source": [
    "### Loading Documents with `Docx2txtLoader`\n",
    "\n",
    "DOCX files can be loaded using `Docx2txtLoader`,\n",
    "which extracts raw text from Word documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f98a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_docx = Docx2txtLoader(\"../../data/docs/Introduction_to_Data_and_Data_Science.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f723fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader_docx.load()\n",
    "for i in range(len(pages)):\n",
    "    pages[i].page_content = ' '.join(pages[i].page_content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb22280",
   "metadata": {},
   "source": [
    "## Step 2: Document Splitting\n",
    "\n",
    "Large documents must be split into smaller chunks before embedding.\n",
    "\n",
    "Smaller chunks:\n",
    "- Improve retrieval accuracy\n",
    "- Fit within model context limits\n",
    "- Preserve semantic coherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa74b5",
   "metadata": {},
   "source": [
    "### Character-Based Text Splitting\n",
    "\n",
    "Character-based splitting divides text using a fixed chunk size\n",
    "and optional overlap to preserve context between chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c030d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splitter = CharacterTextSplitter(separator=\".\", chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_chat_split = char_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e21ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_chat_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_chat_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae51ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_chat_split[16].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d908f",
   "metadata": {},
   "source": [
    "### Markdown Header-Based Text Splitting\n",
    "\n",
    "When documents contain structured headers, a markdown-aware splitter\n",
    "can be used to preserve logical sections such as titles and headings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2_docx = Docx2txtLoader(\"../../data/docs/Introduction_to_Data_and_Data_Science_2.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages2 = loader2_docx.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc18171",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on = [(\"#\", \"Course Title\"), (\"##\", \"Lecture Title\")] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96417085",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_md_split = md_splitter.split_text(pages2[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pages_md_split)):\n",
    "    pages_md_split[i].page_content = ' '.join(pages_md_split[i].page_content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b14258",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_chat_split2 = char_splitter.split_documents(pages_md_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f71fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_chat_split2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b201ac",
   "metadata": {},
   "source": [
    "## Step 3: Text Embedding\n",
    "\n",
    "After splitting, text chunks are converted into dense vector embeddings.\n",
    "\n",
    "These embeddings capture semantic meaning and enable\n",
    "similarity search during retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d8884",
   "metadata": {},
   "source": [
    "### Generating Embeddings with OpenAI\n",
    "\n",
    "Each text chunk is mapped to a high-dimensional vector representation\n",
    "using an embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56984be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = embeddings.embed_query(pages_chat_split2[3].page_content)\n",
    "vector2 = embeddings.embed_query(pages_chat_split2[5].page_content)\n",
    "vector3 = embeddings.embed_query(pages_chat_split2[18].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e67e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vector1), len(vector2), len(vector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c20ffc",
   "metadata": {},
   "source": [
    "### Measuring Similarity Between Embeddings\n",
    "\n",
    "Cosine similarity (via dot product and vector norms)\n",
    "is commonly used to measure semantic similarity between text chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(vector1, vector2), np.dot(vector1, vector3), np.dot(vector2, vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f73a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(vector1), np.linalg.norm(vector2), np.linalg.norm(vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cb1f1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the indexing steps of a RAG pipeline:\n",
    "\n",
    "- Loading documents from PDF and DOCX sources  \n",
    "- Cleaning and normalizing raw text  \n",
    "- Splitting documents into manageable chunks  \n",
    "- Generating vector embeddings for semantic retrieval  \n",
    "- Comparing embeddings using similarity metrics  \n",
    "\n",
    "These steps form the foundation for building\n",
    "retrieval-augmented generation systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchains_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
