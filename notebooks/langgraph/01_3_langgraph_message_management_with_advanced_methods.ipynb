{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220b5366",
   "metadata": {},
   "source": [
    "# LangGraph Message Trimming with `MessagesState` and `RemoveMessage`\n",
    "\n",
    "This notebook demonstrates how **LangGraph manages long-running\n",
    "conversations** using:\n",
    "\n",
    "- `MessagesState` for built-in message accumulation\n",
    "- `add_messages` as the default reducer\n",
    "- `RemoveMessage` for controlled memory trimming\n",
    "- Conditional routing to maintain a bounded context window\n",
    "\n",
    "The goal is to preserve recent conversational context\n",
    "while preventing unbounded memory growth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langgraph.graph import START, END, StateGraph, add_messages, MessagesState\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, RemoveMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from collections.abc import Sequence\n",
    "from typing import Literal, Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe8512",
   "metadata": {},
   "source": [
    "## Message Accumulation with `add_messages`\n",
    "\n",
    "`add_messages` is a reducer that merges:\n",
    "- Existing messages in state\n",
    "- Newly returned messages from a node\n",
    "\n",
    "It ensures conversation history is **appended**, not overwritten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97978561",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = add_messages([HumanMessage(\"Hi! I'm Oscar. \"), \n",
    "                        AIMessage(\"Hey, Oscar! How can I assist you?\")], \n",
    "                        [HumanMessage(\"Could you summarize today's news?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54efda1",
   "metadata": {},
   "source": [
    "## Chat Model Initialization\n",
    "\n",
    "A lightweight deterministic model is used\n",
    "to keep the focus on graph behavior rather than generation quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4288c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\", \n",
    "    temperature=0, \n",
    "    model_kwargs= {\"text\":{\"verbosity\": 'low'}},\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817d6f0",
   "metadata": {},
   "source": [
    "## Defining Graph Nodes\n",
    "\n",
    "Each node:\n",
    "- Receives the full message history\n",
    "- Prints the current state for inspection\n",
    "- Returns new messages to be merged into state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1bf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(state: MessagesState) -> MessagesState:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING ask_question:\")\n",
    "    for i in state[\"messages\"]:\n",
    "        i.pretty_print()\n",
    "    \n",
    "    question = \"What is your question?\"\n",
    "    print(question)\n",
    "    \n",
    "    return MessagesState(messages = [AIMessage(question), HumanMessage(input())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c99aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: MessagesState) -> MessagesState:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING chatbot:\")\n",
    "    for i in state[\"messages\"]:\n",
    "        i.pretty_print()\n",
    "    \n",
    "    response = chat.invoke(state[\"messages\"])\n",
    "    response.pretty_print()\n",
    "    \n",
    "    return MessagesState(messages = [response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a08a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_another_question(state: MessagesState) -> MessagesState:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING ask_another_question:\")\n",
    "    for i in state[\"messages\"]:\n",
    "        i.pretty_print()\n",
    "    \n",
    "    question = \"Would you like to ask one more question (yes/no)?\"\n",
    "    print(question)\n",
    "    \n",
    "    return MessagesState(messages = [AIMessage(question), HumanMessage(input())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2bddd",
   "metadata": {},
   "source": [
    "## Trimming Conversation History\n",
    "\n",
    "To prevent unlimited growth of conversation history,\n",
    "older messages are removed using `RemoveMessage`.\n",
    "\n",
    "This implementation keeps only the **last 5 messages**\n",
    "and removes everything before them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bfdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_messages(state: MessagesState) -> MessagesState:\n",
    "    print(f\"\\n-------> ENTERING trim_messages:\")\n",
    "    \n",
    "    remove_messages = [RemoveMessage(id = i.id) for i in state[\"messages\"][:-5]]\n",
    "    \n",
    "    return MessagesState(messages = remove_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31150daa",
   "metadata": {},
   "source": [
    "## Conditional Routing\n",
    "\n",
    "After each interaction, the user decides whether\n",
    "the conversation should continue.\n",
    "\n",
    "If the user answers `\"yes\"`, old messages are trimmed\n",
    "and the conversation continues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_function(state: MessagesState) -> Literal[\"trim_messages\", \"__end__\"]:\n",
    "    \n",
    "    if state[\"messages\"][-1].content == \"yes\":\n",
    "        return \"trim_messages\"\n",
    "    else:\n",
    "        return \"__end__\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825f0bf",
   "metadata": {},
   "source": [
    "## Building the Graph\n",
    "\n",
    "The graph combines:\n",
    "- Conversational nodes\n",
    "- Conditional looping\n",
    "- Memory trimming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"chatbot\", chatbot)\n",
    "graph.add_node(\"ask_another_question\", ask_another_question)\n",
    "graph.add_node(\"trim_messages\", trim_messages)\n",
    "\n",
    "graph.add_edge(START, \"ask_question\")\n",
    "graph.add_edge(\"ask_question\", \"chatbot\")\n",
    "graph.add_edge(\"chatbot\", \"ask_another_question\")\n",
    "graph.add_conditional_edges(source = \"ask_another_question\", \n",
    "                            path = routing_function)\n",
    "graph.add_edge(\"trim_messages\", \"ask_question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc807083",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compiled = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a56b8",
   "metadata": {},
   "source": [
    "## Executing the Graph\n",
    "\n",
    "The graph starts with an empty message history.\n",
    "Messages are accumulated and trimmed automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf068f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compiled.invoke(MessagesState(messages = []))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b98d1",
   "metadata": {},
   "source": [
    "## `RemoveMessage` Usage Example\n",
    "\n",
    "The following example demonstrates how `RemoveMessage`\n",
    "can be used to selectively remove earlier messages\n",
    "from a conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = add_messages([AIMessage(\"What is your question?\"), \n",
    "                        HumanMessage(\"Could you tell me a grook by Piet Hein?\"),\n",
    "                        AIMessage(\"Certainly! Here's a well-known grook by Piet Hein...\"),\n",
    "                        AIMessage(\"Would you like to ask one more question?\"),\n",
    "                        HumanMessage(\"yes\"),\n",
    "                        AIMessage(\"What is your question?\"),\n",
    "                        HumanMessage(\"Where was the poet born?\"),\n",
    "                        AIMessage(\"Piet Hein was born in Copenhagen, Denmark, on December 16, 1905.\"),\n",
    "                        AIMessage(\"Would you like to ask one more question?\")],\n",
    "                       [HumanMessage(\"yes\")]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17895e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "remvove_message = [RemoveMessage(id = i.id) for i in my_list[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d645d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "remvove_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ebd28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "- Using `MessagesState` for built-in message accumulation\n",
    "- Trimming conversation history with `RemoveMessage`\n",
    "- Implementing sliding-window memory\n",
    "- Conditional routing with stateful conversations\n",
    "- Managing long-running chat interactions safely\n",
    "\n",
    "This pattern mirrors real-world conversational agents,\n",
    "where memory must be controlled without losing relevance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langraph_env",
   "language": "python",
   "name": "langraph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
