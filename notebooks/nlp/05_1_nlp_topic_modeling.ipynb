{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf5ded3",
   "metadata": {},
   "source": [
    "## üß© Topic Modeling\n",
    "*(Personal Practice Notes)*\n",
    "\n",
    "Topic modeling is an NLP technique used to discover **hidden themes (topics)**\n",
    "within a collection of documents.\n",
    "\n",
    "Documents can be:\n",
    "- rows in a DataFrame\n",
    "- items in a list\n",
    "- individual text files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72120dad",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ What is Topic Modeling?\n",
    "\n",
    "Topic modeling scans a collection of documents to identify **patterns of word usage**.\n",
    "\n",
    "Based on these patterns, documents that discuss similar ideas\n",
    "are grouped together into **topics**.\n",
    "\n",
    "Key characteristics:\n",
    "- Topic modeling is an example of **unsupervised learning**\n",
    "- No labeled data is required\n",
    "- Algorithms discover structure automatically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1702",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2Ô∏è‚É£ Why Topic Modeling Works\n",
    "\n",
    "Topic modeling algorithms:\n",
    "- identify recurring word patterns\n",
    "- learn what each document is mostly about\n",
    "- group documents that share similar word distributions\n",
    "\n",
    "This helps uncover the **main themes**\n",
    "that run through a collection of documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b37a56",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Common Topic Modeling Algorithms\n",
    "\n",
    "Two widely used topic modeling techniques are:\n",
    "\n",
    "1. **Latent Dirichlet Allocation (LDA)**\n",
    "2. **Latent Semantic Analysis (LSA)**\n",
    "\n",
    "Both aim to uncover latent topics, but they use different mathematical approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ec937",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is a probabilistic topic modeling technique.\n",
    "\n",
    "Key ideas:\n",
    "- Each document is a mixture of topics\n",
    "- Each topic is a mixture of words\n",
    "- Topics are inferred based on word co-occurrence patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries we will use is gensim, pandas, re, nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import LsiModel\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b157852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/news_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#title column contains the news article titles and content column contains the main news article complete\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning and Cleaning the text data\n",
    "articles = data[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles =  articles.str.lower().apply(lambda x: re.sub(r\"([^\\w\\s])\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712dfc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "articles = articles.apply(lambda x: ' '.join(word for word in x.split() if word not in en_stopwords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca897dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the text\n",
    "articles = articles.apply(lambda x: word_tokenize(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming the text   #large amount of text, stemming is chosen here to speed up processing\n",
    "ps = PorterStemmer()\n",
    "articles = articles.apply(lambda tokens: [ps.stem(token) for token in tokens]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ccfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary and corpus needed for Topic Modeling\n",
    "dictionary = corpora.Dictionary(articles)  #each word in the articles is assigned a unique id which will alow the lda model to access the words \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f811692",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term = [dictionary.doc2bow(doc) for doc in articles]  #converting each document into the bag-of-words format\n",
    "#doc2bow takes a single article, looks up each word in the dicttionary and returns a list of words and their frequency in the article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42186240",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to begin modeling we decide how many topics we want to extract from the articles. For this example, we choose 2 topics.\n",
    "num_topics = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce5e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(corpus=doc_term, id2word=dictionary, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44185299",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics(num_topics=num_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a622604",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d0e1a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Latent Semantic Analysis (LSA)\n",
    "\n",
    "LSA is based on **linear algebra** and **dimensionality reduction**.\n",
    "\n",
    "It identifies similarities between documents using:\n",
    "- clustering\n",
    "- similarity scores\n",
    "\n",
    "LSA rests on two key ideas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec690c2",
   "metadata": {},
   "source": [
    "### üîπ Distributional Hypothesis\n",
    "\n",
    "The distributional hypothesis states:\n",
    "\n",
    "> Words with similar meanings tend to appear in similar contexts.\n",
    "\n",
    "In other words:\n",
    "- words that frequently occur together\n",
    "- often share related meanings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388ea5a",
   "metadata": {},
   "source": [
    "### üîπ Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD reduces high-dimensional text data into a lower-dimensional space\n",
    "that captures the most important patterns.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "M = U Œ£ V·µÄ\n",
    "\n",
    "Where:\n",
    "- **M** = document-term matrix\n",
    "- **U** = document-topic matrix\n",
    "- **Œ£ (Sigma)** = importance of each latent topic\n",
    "- **V·µÄ** = topic-term matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61387176",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Building an LSA Model\n",
    "\n",
    "We use the **LSI (Latent Semantic Indexing)** implementation of LSA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsmodel =  LsiModel(doc_term, num_topics=num_topics, id2word=dictionary)\n",
    "print(lsmodel.print_topics(num_topics=num_topics, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d30968",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Determining the Optimal Number of Topics\n",
    "\n",
    "Choosing the correct number of topics is critical.\n",
    "\n",
    "We use **topic coherence** to evaluate topic quality.\n",
    "\n",
    "Coherence measures:\n",
    "- how meaningful the top words in a topic are\n",
    "- how often those words appear together in documents\n",
    "\n",
    "Higher coherence scores indicate more interpretable topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "\n",
    "for num_topics_i in range(min_topics, max_topics+1):\n",
    "    model = LsiModel(doc_term, num_topics=num_topics_i, id2word=dictionary, random_seed=0)\n",
    "    model_list.append(model)\n",
    "    coherence_model  = CoherenceModel(model=model, texts=articles, dictionary=dictionary, coherence='c_v')   #cv here measures how often the top words of a topic \n",
    "    #appear together in the documents\n",
    "    coherence_values.append(coherence_model.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6acac",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Visualizing Coherence Scores\n",
    "\n",
    "We plot coherence scores against the number of topics\n",
    "to identify the optimal topic count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd89f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(min_topics, max_topics+1), coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba64a6",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Training the Final LSA Model\n",
    "\n",
    "Based on coherence scores, we select the optimal number of topics\n",
    "and train the final LSA model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_n_topics = 3\n",
    "lsmodel_f = LsiModel(doc_term, num_topics=final_n_topics, id2word=dictionary)\n",
    "print(lsmodel_f.print_topics(num_topics=final_n_topics, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a986e",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Takeaways\n",
    "\n",
    "- Topic modeling uncovers hidden themes in text\n",
    "- It is an unsupervised learning technique\n",
    "- LDA uses probabilistic modeling\n",
    "- LSA uses linear algebra and dimensionality reduction\n",
    "- Topic coherence helps evaluate topic quality\n",
    "- Selecting the right number of topics is crucial for interpretability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
